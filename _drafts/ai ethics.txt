ai ethics


Practitioners need to wake up. This is going to be regulated. Do we want it regulated by 1. people who do understand it or 2. people who do.



different across genders


do people want the probability of success to be higher

if not, then that's the problem


What to do with gender in-specific pronouns?

Google has given both options, which is a good solution

Originally they didn't



I think it's important to have an idea of what would a fix look like. Sure, sometimes just pointing out a problem is useful, but in many cases, and specifcally in these because they're going to have to many problems and solutions are not going to be straightforward.

It's a bit like saying there's a problem with rascism in the world. THere's just not much additional value in pointing it out. We need to be at the solution phase.

Could we do something complex?
There are ways to teach a GAN to find and remove racism from algorithms, but there's not way to get the public to understand what a GAN is.



What questions should we ask:
Did your parents go to jail? Do your friends do drugs?
Are your parents divorced?


It's not clear how the data got it so wrong? These models don't seem that complex, so why aren't they better? Should they all be open source? Should the training data all be open source?


It's WAY to easy for humans who don't understand these algorithms to think they are capable of more than they are. For example, when you read an email, you can be confident that that is what the sender sent. The chance is a word got changed in the email process is basically 0. So people trust computers. But the same does not apply to predictive algorithms. When 538 says there's a 68? percent change Hillary will when, or there's a 50% chance this person will commit another crime, there are TWO differences

The first is that they are probabilistic, even if they were perfect.

The second is that they're not perfect, and aren't going to be any time soon.


# Should algos be held to a higher standard?

Yes - they are used at incredible scale. There should be careful reviews. And also appeals and the like



The onous isn't just on the algorithm creator though. It's on the people using and and implementing it and selling it. The entire chain needs to be involved in these processes. The person who knows the data and wrote the algorithm (and anyone worth their salt who's writting a production-level algorithm will become intimately familiar with the data) is best placed to guide it's usage. So those other people must not firewall that person off. Sales people have to make sure to make that person avilable to end customers. And ensure that that person understands how end customers are using the data.








What if conspiracy theory videos are popular? What if an algo finds that something is dumb (without understanding at all what they're doing) and will accept any video. So it jsut sends them crazier and crazier stuff because they keep clicking. How do you fix this? I have worked with elderly people who I felt like were taken advantage of in a photography store and in buying a laptop. They have all these features they couldn't use and didn't understand and pieces of equipment that made their lives harder.
Could the algo cluster videos and find conspiracy theories? Could the humans label these?



# How to prevent runaway feedback mechanisms?
If you recommend a meetup to more men, more men will join, then you'll recommend it more and more... how to break this cycle?


# Things could be made to help or break humans

Cars used it be less safe and people thought it was the drivers. Yes, it was, but you could still add some safety features.
The intersection below work absolutely breaks humans




articles:

https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing



o bir doktor. o bir hemşire. o bir doktor. o bir hemşire. o bir tamircidir. o bir tamircidir. o bir profesör. o bir doktor.



## AI opens up new possibilities

- like other technology, cloning...


## AI makes things easy


## There's a lot going on that we've never really answered

Is it OK to post someone else's picture online without their consent? People do it all the time, so I guess we decided it's OK (in the way that societies decide things)... What about a slightly embarassing picture? What about a REALLY embarrassing picture? Or a "you could get fired for this" picture? 
OK, then what about face swaps? Is it OK if I do a face swap and then post a picture? AI makes this easy. I could do a face swap and make it look like I'm hanging out with a celebrity (maybe face swap in Lassie?)

You could do this before. But AI makes it easy. You used to need to go frame by frame, but not any more... Was it OK before? Is it OK now?


responsibility:

no one feels responsible. First, this happens *everywhere*. I've worked on projects that didn't go well and the person who was solely responisble for it didn't feel responsible. OK, but this is a case where it's even worse with machine learning. In machine learning the engineer can claim that it's the data that made the decision, which defuses the blame. This is not acceptable.


The media has been horrible in this space. Their understanding of AI is not much more than knowing that articles with AI in the title get more clicks and they get an excuse to use a stock photo from Terminator.


Face ID - exact same thing that happens at the grocery store. They basically say they'll pay us for our shopping information. They don't use face id because it creeps people out.



i used to think it was such a joke that a cat detector would detect a dog. but i realize that i was misunderstanding the trust people put in computer systems. i was wrong, this is a real problem. for me, pouring deep into failed results from machine learning, when I see a wrong prediction, I think "Yea, I know... I'm trying to fix it. This stuff is still not very good". I thought it was a joke. But now I realize I was wrong. Because other people don't take it as a joke.When they see a computer print "likely repeat offender", they think it's true. they don't realize how nonsense it is.
ai ethics - again, this is software engineering, but remember that this has big consequences. some of the results show thatthis is just garbage software engineering. many times that's not a big deal - some app dies and i have to restart it. but we have to clear-eyed about the impact we are having.i read some studies, and i just think. well, these people suck at basic machine learning. the problems are not difficult to test. (by the i mean there are basic statistical tests that should have caught these issues).Some of it is going to the cheapest bidder. What can you do if another company has no idea what they're doing and underbids you. You're responsible - you knew how to test the product - and you priced it accordingly. but you lost to someone who clearly had no idea what they were doing.that means the buyer must be smarter and the seller must realize the importance of their actions. a buyer should know, when buying a piece of custom software, you need to know a bit about what you're doing. this isn't like buying a car. this is custom stuff.- these are used at scale -> they have more impact

Runaway feedback loops - maybe only redheads like ice cream. so you advertise it to them.- predictive policing is absolutely going to have this problem.

accountability?accountability is tricky. when something goes wrong, it is often because a multitude of small mistakes.an ai practitioner builds an algo for a certain distribution, and it's doesn't generalize well. perhaps they could have made a better algo. or maybe they could have 

imagine a cat and dog detector and you have a line between them. what if something is far away from the line? what if it doesn't match the particular distribution?- there is no way for the model to tell you unless you give it that option.

there can be true bugsbad dataincorrect assumptionsincorrect extrapolation (by humans or by the model because it doesn't have a chance)

people blame the algorithm, but I think it's going to find a way to optimize. The question is should it optimize(link to capitalism everywhere blog)?
for the do we need capitalism everywhere blog:
an AI could find a way to make the most money off people while doing the least
https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsyTo me, this is an entirely predictable result. If this wasn't the point, then what was the point? Was this AI going to decide that what it really wanted to do was help people so it came up with a different algo? That's the nonsense of TV. Algos don't do that. You're solving an optimization problem. Be realistic.
Here's how I read this story: A company decided to maximize their profits from providing health care, so they used an algorithm called a neural network to optimize the amount they charged while reducing what they provided. And then the media says "Bad algorithm!!! AI so scary!!!!" This is fucking nonsense to blame the algorithm. But it definitely results in a lot of clicks, so here we are. (First, provide a simple to understand version of an optimization algorithm).(Although in this particular case it looks like there was a software bug)
- so this story goes to knowing how important your work is.
ythefuck was the person fired?- this boils down to the extra trust thing. it seems laughable to me that anyone would trust these algorithms beyond their capability, but they do. 
it's kind of like the lap coat phonemenon, where you see people wearing labcoats on TV to bulk their credibility. a computer has ultimate credibility for some people.


no one feels responsible, and for good reason. that's why it's such a difficult problem that requires attention.Data cleaning is the responsibility of the ml practitioner. You are responsible for thouroughly understanding your data. I spend most of my time messing with data. It's noisy and downright wrong many times. I run sanity check after sanity check on it. Even for a low consequence application (e.g. a hot dog not hot dog phone app), I would delve deeply into the data. As the importance of your work increases, so must your knowledge of the data. but the baseline is still really high. I think most people would be surprised by how much of the time of machine learning engineers consists of data cleaning.Here's an example of data not being cleaned well enough: https://www.latimes.com/local/lanow/la-me-ln-calgangs-audit-20160811-snap-story.htmlTHE ML ENGINEER IS RESPONSIBLE FOR THE DATA THAT GOES INTO THE ALGORITHM, UNLESS ANOTHER ML ENGINEER IN YOUR COMPANY VOUCHES FOR IT

Have an honest debate - if you criticize a facial recognition algorithm for being worse with black people, then imagine it weren't. would you support it then? if not, you're being disingenuous by saying you don't support it because of the racial reason. you don't like it at all, so be honest and make *that* argument.
some of the code is complete garbage: https://www.theverge.com/2018/7/26/17615634/amazon-rekognition-aclu-mug-shot-congress-facial-recognition

we can have ai solutions for some things, like gans that find if you're penalizing the word 'woman' (https://www.theverge.com/2018/10/10/17958784/ai-recruiting-tool-bias-amazon-report), or zip codes associated with black people. but in general, i think we put too much emphasis on 

does facebook advertise to terrorists?- youtube will be tempted to put people down rabbit holes
it's important to focus. let's focus on ai. there is a lot of criticism with capitalism and society and other things, but i find that i only make progress thinking through an issue when i focus clearly on it. so the focus of this post is on ai ethics. * note: ai used in the media way, not in the real way. whether ai can become conscious is far off from today.

what to hold companies accountable for? there will be mistakes. some of these seem obvious though
- let's say fb does advertise  to terrorists... what to do about it?- or if youtube turns a bunch of people into nutjobs


i find it's too easy to just say "we need common sense reform". everyone celebrates that message but it's basically an empty statement. i'll try to be more specific. I don't like when conclusions are so vague that 100% of people will agree with them.

just like i'm a software engineer and dont want software involved in our elections









You have to be aware of what your model could possibly learn

If people are "liking" posts and your finding latent characteristics of those posts and promoting those with those characteristics, you don't know what you're promoting. You could be promoting something awful. There are things that aren't illegal that are still undesireable. Somewhere, there's a line on what constitutes child pornography. Just next to that line, on the legal side, is an image that's very close to illegal child pornography but isn't - perhaps it's a suggestive photograph or something like that. Would you want an algorithm to promote that? I don't think so. We shouldn't be promoting latent features that we don't understand. In some cases, the scope is constrained. If you are promoting types of apples to people based on their profiles, you're either going to promote a Granny Smith or a Honeycrisp, and there's no real harm from using variables you don't understand.

But if the scope is unlimited, like in unrestricted speech or images, we can't be promoting unknown features. The room for this to go bad is too great.

It's certainly true that models trained on machine learning are a reflection of the society that generated the training data. But just because it's a reflection doesn't mean it's OK to promote whatever the crowd would like.





